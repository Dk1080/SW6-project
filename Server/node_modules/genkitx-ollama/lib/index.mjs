import { logger } from "genkit/logging";
import {
  GenerationCommonConfigSchema,
  getBasicUsageStats
} from "genkit/model";
import { genkitPlugin } from "genkit/plugin";
import { defineOllamaEmbedder } from "./embeddings.js";
function ollama(params) {
  return genkitPlugin("ollama", async (ai) => {
    const serverAddress = params.serverAddress;
    params.models?.map(
      (model) => ollamaModel(ai, model, serverAddress, params.requestHeaders)
    );
    params.embedders?.map(
      (model) => defineOllamaEmbedder(ai, {
        name: model.name,
        modelName: model.name,
        dimensions: model.dimensions,
        options: params
      })
    );
  });
}
function ollamaModel(ai, model, serverAddress, requestHeaders) {
  return ai.defineModel(
    {
      name: `ollama/${model.name}`,
      label: `Ollama - ${model.name}`,
      configSchema: GenerationCommonConfigSchema,
      supports: {
        multiturn: !model.type || model.type === "chat",
        systemRole: true
      }
    },
    async (input, streamingCallback) => {
      const options = {};
      if (input.config?.temperature !== void 0) {
        options.temperature = input.config.temperature;
      }
      if (input.config?.topP !== void 0) {
        options.top_p = input.config.topP;
      }
      if (input.config?.topK !== void 0) {
        options.top_k = input.config.topK;
      }
      if (input.config?.stopSequences !== void 0) {
        options.stop = input.config.stopSequences.join("");
      }
      if (input.config?.maxOutputTokens !== void 0) {
        options.num_predict = input.config.maxOutputTokens;
      }
      const type = model.type ?? "chat";
      const request = toOllamaRequest(
        model.name,
        input,
        options,
        type,
        !!streamingCallback
      );
      logger.debug(request, `ollama request (${type})`);
      const extraHeaders = requestHeaders ? typeof requestHeaders === "function" ? await requestHeaders(
        {
          serverAddress,
          model
        },
        input
      ) : requestHeaders : {};
      let res;
      try {
        res = await fetch(
          serverAddress + (type === "chat" ? "/api/chat" : "/api/generate"),
          {
            method: "POST",
            body: JSON.stringify(request),
            headers: {
              "Content-Type": "application/json",
              ...extraHeaders
            }
          }
        );
      } catch (e) {
        const cause = e.cause;
        if (cause && cause instanceof Error && cause.message?.includes("ECONNREFUSED")) {
          cause.message += ". Make sure the Ollama server is running.";
          throw cause;
        }
        throw e;
      }
      if (!res.body) {
        throw new Error("Response has no body");
      }
      let message;
      if (streamingCallback) {
        const reader = res.body.getReader();
        const textDecoder = new TextDecoder();
        let textResponse = "";
        for await (const chunk of readChunks(reader)) {
          const chunkText = textDecoder.decode(chunk);
          const json = JSON.parse(chunkText);
          const message2 = parseMessage(json, type);
          streamingCallback({
            index: 0,
            content: message2.content
          });
          textResponse += message2.content[0].text;
        }
        message = {
          role: "model",
          content: [
            {
              text: textResponse
            }
          ]
        };
      } else {
        const txtBody = await res.text();
        const json = JSON.parse(txtBody);
        logger.debug(txtBody, "ollama raw response");
        message = parseMessage(json, type);
      }
      return {
        message,
        usage: getBasicUsageStats(input.messages, message),
        finishReason: "stop"
      };
    }
  );
}
function parseMessage(response, type) {
  if (response.error) {
    throw new Error(response.error);
  }
  if (type === "chat") {
    return {
      role: toGenkitRole(response.message.role),
      content: [
        {
          text: response.message.content
        }
      ]
    };
  } else {
    return {
      role: "model",
      content: [
        {
          text: response.response
        }
      ]
    };
  }
}
function toOllamaRequest(name, input, options, type, stream) {
  const request = {
    model: name,
    options,
    stream
  };
  if (type === "chat") {
    const messages = [];
    input.messages.forEach((m) => {
      let messageText = "";
      const images = [];
      m.content.forEach((c) => {
        if (c.text) {
          messageText += c.text;
        }
        if (c.media) {
          images.push(c.media.url);
        }
      });
      messages.push({
        role: toOllamaRole(m.role),
        content: messageText,
        images: images.length > 0 ? images : void 0
      });
    });
    request.messages = messages;
  } else {
    request.prompt = getPrompt(input);
    request.system = getSystemMessage(input);
  }
  return request;
}
function toOllamaRole(role) {
  if (role === "model") {
    return "assistant";
  }
  return role;
}
function toGenkitRole(role) {
  if (role === "assistant") {
    return "model";
  }
  return role;
}
function readChunks(reader) {
  return {
    async *[Symbol.asyncIterator]() {
      let readResult = await reader.read();
      while (!readResult.done) {
        yield readResult.value;
        readResult = await reader.read();
      }
    }
  };
}
function getPrompt(input) {
  return input.messages.filter((m) => m.role !== "system").map((m) => m.content.map((c) => c.text).join()).join();
}
function getSystemMessage(input) {
  return input.messages.filter((m) => m.role === "system").map((m) => m.content.map((c) => c.text).join()).join();
}
export {
  ollama
};
//# sourceMappingURL=index.mjs.map